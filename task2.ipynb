{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "#### Student Name: PRASHANT JAJORIA\n",
    "#### Student ID: 31187366\n",
    "\n",
    "Date: 13/09/2020\n",
    "\n",
    "Environment: Python 3.7.4 and Anaconda 4.8.4 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* pandas - for reading excel into Dataframe. And dataframe manuplation.\n",
    "* langid - for finding English tweets, included in Anaconda Python 3.7.4\n",
    "* nltk - (Natural Language Toolkit, included in Anaconda Python 3.7.4)\n",
    "\n",
    "* nltk.collocations - for finding bigrams, included in Anaconda Python 3.7.4\n",
    "* nltk.tokenize - for Regex tokenization and Multiple word expression tokenization, included in Anaconda Python 3.7.4\n",
    "* nltk.probability - for calculating frequency distribution of tokens and finding most common unigrams and bigrams, included in Anaconda Python 3.7.4\n",
    "* sklearn.feature_extraction.text - for calculating count vector using the `CountVectorizer`\n",
    "* nltk.util - for calculating the bigrams using the `ngrams` function\n",
    "* nltk.stem - for stemming of tokens using the PorterStemmer\n",
    "\n",
    "## 1. Introduction\n",
    "The main goal of this assignment is to  convert exracted data to into a proper format, which can be used by down stream data analysis algorithms. An excel file with 81 sheets is provided with Tweets about Covid-19 for different dates. The main objective in this task is to tokenize the data and get the data in format that can be used by downstream data analyais algorithms .\n",
    "\n",
    "\n",
    "Following are the requirement of the task:\n",
    "1. Only English tweets should be processed for tokenization. \n",
    "2. Generate a vocabulary file with the tokens sorted alphabitacilly.\n",
    "3. For each day, to calculate the top 100 unigrams and generate a top 100 unigrams text file \n",
    "4. For each day, to calculate the top 100 bigrams and generate a top 100 bigrams text file \n",
    "5. Generate a sparse representation (i.e., doc-term matrix) of the excel file with provided format.\n",
    "\n",
    "A step by step explanation for completing the requirements will be explained in the following code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import langid\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading the Excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the Excel file using the Pandas `ExcelFile` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading my excel file\n",
    "file = pd.ExcelFile(\"31187366.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the name of each sheet of the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the sheet names of excel file\n",
    "sheets = file.sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring variable that will store the concatenated Tweets text for each date in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring dictionary to store the tweets\n",
    "dict_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parsing, cleaning and filtering Non English tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reading each sheet of Excel file using the `parse` method and storing it as a Dataframe. \n",
    "- Removing all the NA columns and rows using the `dropna` with `how` = 1 for columns , and `how` = 0 for rows.\n",
    "- Resetting the column name for each data frame conditionally using the `reset_index` function. And dropping the old column names from the dataframe.\n",
    "- Using `apply` to use the `lambda` function along with `langid.calssify` fucntion on `Text` column to have only English Tweets in the data frame.\n",
    "- Getting the all the values of the Text column and joining using the new line character `\\n` using the `join` function.\n",
    "- Finally storing the values of each date in a Dictionary with key being the Date and value is the concatenated string having all the tweet text of that date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# looping each sheet\n",
    "for sheet in sheets:\n",
    "    \n",
    "    #print(sheet)\n",
    "    \n",
    "    # get each sheet\n",
    "    df = file.parse(sheet_name = sheet)\n",
    "    \n",
    "    # remove all rows and columns with NA values\n",
    "    df = df.dropna(0,how=\"all\")\n",
    "    df = df.dropna(1,how=\"all\")\n",
    "    \n",
    "    \n",
    "    if(df.columns[0] != \"text\" and df.columns[1] != \"id\" and df.columns[2] != \"created_at\" ):\n",
    "        # changing the column names\n",
    "        df.columns = df.iloc[0]\n",
    "        \n",
    "        # removing old colmuns\n",
    "        df.drop(df.index[0],inplace = True)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # classify the tweets and store the result TRUE/FALSE in an column \"result\"\n",
    "    df[\"result\"] = df['text'].apply(lambda x : langid.classify(str(x)))\n",
    "    \n",
    "    # remove the non-english tweets\n",
    "    df = df[df[\"result\"].apply(lambda x : x[0] == \"en\")]\n",
    "        \n",
    "    # get the values column\n",
    "    tweet_values = df.text.values\n",
    "    \n",
    "    # store in list if the text is string\n",
    "    tweets = [ tweet for tweet in tweet_values if isinstance(tweet, str)]\n",
    "    \n",
    "    # join all the tweets and store as dictionary\n",
    "    dict_data[sheet] = '\\n'.join(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization using Regexp Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the Regexp Tekenizer form nltk.tokenize pacakage with the provided Regular Expression `[a-zA-Z]+(?:[-'][a-zA-Z]+)?` to tokenize each concatenated string of a date.\n",
    "- Following is the explanation of each part of the Regular Expression:\n",
    "    - `[a-zA-Z]` : Accepts atleast one or more upper or lower case or mix of both characters.\n",
    "    -  `(?:[-'][a-zA-Z]+)?` : This is an optional non capturing group. It captures words that start with either `-` or `'`, followed by atleast one or more upper or lower case or mix of both case characters.\n",
    "- The result of Tokenization is converted to lower case and stored in a Dictionary with key as the Date and value as the list of Tokens of that date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the tweets\n",
    "dict_unigram_token = {}\n",
    "\n",
    "# regex tokenizer\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "\n",
    "# do the tokenization for each date\n",
    "for date, tweets in dict_data.items():\n",
    "    \n",
    "    # lowercase the string\n",
    "    tweets = tweets.lower()\n",
    "    \n",
    "    # store the unigram tokens in a dictionary\n",
    "    dict_unigram_token[date] = tokenizer.tokenize(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculating the top 100 frequent Bigarms for each Day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization we calculate the Bigrams for each day first. This step need tobe carried out before Stemming of tokens, as after stemming the words will lose it meaning due to characters being taken off, and we are only left with the root word.\n",
    "\n",
    "Using the `ngrams` function from NLTK pacjakage allows us to calculate the Bigrams by specifying the parameter size as `n=2`.\n",
    "\n",
    "Using the result then we do a frequency distribution to get the Frequency of each Bigram in the documents.\n",
    "\n",
    "After this the using the `most_common` function we get the top 100 Bigrams from all the sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATING 100 FREQUENT BIGRAMS FOR A DAY\n",
    "\n",
    "dict_top_100_bigrams = {}\n",
    "\n",
    "for date, list_unigram_tokens in dict_unigram_token.items():\n",
    "    \n",
    "    bigrams = ngrams(list_unigram_tokens, n = 2)\n",
    "    fdbigram = FreqDist(bigrams)\n",
    "    \n",
    "    dict_top_100_bigrams[date] = fdbigram.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write the top 100 frequent Bigrams to .txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having the top 100 Bigrams, we write those to the `31187366_100bi.txt` file using the format `Bigram:Count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the top 100 BIGRAM file\n",
    "out_file = open(\"./final_op/31187366_100bi.txt\", 'w')\n",
    "\n",
    "for k,v in dict_top_100_bigrams.items():\n",
    "        out_file.write('{}:{}'.format(k,v) + '\\n')\n",
    "        \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Removing tokens with length less than 3 from the vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one of the requiement of the task we do not process tokens of size less than 3. So removing those from the vocab.\n",
    "\n",
    "Using List comprehension to make a new list of Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing tokens with length less than 3\n",
    "for date, tokens in dict_unigram_token.items():\n",
    "    dict_unigram_token[date] = [ word for word in filter(lambda x : len(x) >= 3,tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reading the Stopwords file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we see the tokens, we find that most of the tokens are common English grammar words that are of no meaning independently. These are Stop words and are not imortant for our analysis and are removed from the vocublary. These stopwords are called **Context Independent Stopwords.**\n",
    "\n",
    "Reading the given `stopwords_en.txt` file to get the stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of stop words\n",
    "stopwords = []\n",
    "\n",
    "#open the stopwords file\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    \n",
    "    # read the stop word line by line\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Removing Context Independent Stop words from the vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the given stopwords file we check the membershipof each token in the Stop words list. And we make vocab using List comprehension after removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT INDEPENDENT STOP WORDS REMOVAL\n",
    "\n",
    "# make a new dictionary after removing the stop words\n",
    "dict_unigram_token_no_stop_words = {}\n",
    "\n",
    "# convert to set\n",
    "stopwords_set = set(stopwords)\n",
    "\n",
    "# for each sheet, reomove the stop words\n",
    "for sheet in sheets:\n",
    "    \n",
    "    # store it in a new dictionary\n",
    "    dict_unigram_token_no_stop_words[sheet] = [word for word in dict_unigram_token[sheet] if word not in stopwords_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Calculating the Frequency distribution of Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the Frequency Distribution of all the words after removing the Stopwords. Using `FreqDist()` to do this frequency Ditribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allwords after reomving the words with less than 3\n",
    "list_no_stop = list(chain.from_iterable([set(value) for value in dict_unigram_token_no_stop_words.values()]))\n",
    "fd_no_stop = FreqDist(list_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting words that appear in less than 5 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous Frequency Distribution object obtained, we make a list of words that appear in less than 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the words which appera in less than 5 documents\n",
    "doc_ref_less_5 = set([w[0] for w in fd_no_stop.items() if w[1] < 5])\n",
    "#doc_ref_less_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting words that appear in more than 60 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous Frequency Distribution object obtained, we make a list of words that appear in more than 60 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the words which appera in more than 60 documents\n",
    "doc_ref_more_60 = set([w[0] for w in fd_no_stop.items() if w[1] > 60])\n",
    "#doc_ref_more_60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Removing Context Dependent Stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the preiously created List of Context independent tokens of frequency less than 5 and frequency more than 60, we check for Membership f each token in both the list, and make a new List using List comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT DEPENDENT STOP WORDS REMOVAL\n",
    "\n",
    "# new dictionary after removing the less common words\n",
    "dict_unigram_token_less_freq_rem = {}\n",
    "\n",
    "# for each sheet, reomove the words which appears in less than 5 documents\n",
    "for sheet in sheets:\n",
    "    \n",
    "    # store it in a new dictionary\n",
    "    dict_unigram_token_less_freq_rem[sheet] = [w for w in dict_unigram_token_no_stop_words[sheet] if w not in doc_ref_less_5 and w not in doc_ref_more_60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Stemming using Porter stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **PorterStemmer** defined in the NLTK.stem package, we stem all the tokens and get a new vocab. We can do the Stemming now as all the Context dependent and context Independent tokensa are removed. And also all the Bigrams for each day are already calculated.\n",
    "\n",
    "Using the `stem` function we carry out stemming of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEMMING\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "dict_stemmed_unigram_tokens = {}\n",
    "\n",
    "for k,v in dict_unigram_token_less_freq_rem.items():\n",
    "    dict_stemmed_unigram_tokens[k] = [ stemmer.stem(w) for w in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of all Stemmed tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `from_iterable()` method from `chain` pacakge to create a List of all the Stemmed tokens from every file.\n",
    "\n",
    "Using a `set()` to keep only one instance of the token for a day in the final tokens list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = list(chain.from_iterable([set(value) for value in dict_stemmed_unigram_tokens.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Calculating the top 100 frequent unigrams for each day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the tokens are stemmed, the vocabulary is reduced keeping only the root words and other important words that are in the documents.\n",
    "\n",
    "We calculate the Frequency distribution of tokens of each day after stemming. This gets the frequency of tokens of each day.\n",
    "\n",
    "Again we use the `most_common()` function to get the top 100 Unigrams of each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the top 100 unigrams for each day\n",
    "dict_100_unigram_each_day = {}\n",
    "\n",
    "# new dict ot store the top 100 unigrams token\n",
    "dict_unigrams = {}\n",
    "\n",
    "for sheet in sheets:\n",
    "    freqDist = FreqDist(dict_stemmed_unigram_tokens[sheet])\n",
    "    dict_100_unigram_each_day[sheet] = freqDist.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Write the top 100 frequent Unigrams to .txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the top 100 unigarms to the `31187366_100uni.txt` file. We use the format of `Unigram:Count` for writitng to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the top 100 UNIGRAM file\n",
    "out_file = open(\"./final_op/31187366_100uni.txt\", 'w')\n",
    "\n",
    "for k,v in dict_100_unigram_each_day.items():\n",
    "        out_file.write('{}:{}'.format(k,v) + '\\n')\n",
    "        \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Calculating 200 meaningful Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can proceed with the task of finding the most meaningful Bigrams from all the tokens, as the Context Dependent and Context Independentstop words are already removed.\n",
    "\n",
    "We use the `BigramCollocationFinder` defined in the `nltk.collocations` package to get the Bigrams.\n",
    "\n",
    "Then using the result we got from the `BigramCollocationFinder` we use `PMI` as an association measure to know how important the Bigrams are in the documents.\n",
    "\n",
    "We extract to top 200 best Bigrams using the `nbest` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATING 200 MEANINGFUL BIGRAMS FROM ALL THE DAYS\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(fd_no_stop)\n",
    "\n",
    "top_200_bigrams = finder.nbest(bigram_measures.pmi, 200)\n",
    "\n",
    "#top_200_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retokenize using MWETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aa we have the Bigrams that are meaningful, we now want to inlcude them in our vocublary. For theis we have to retokenize the vocab.\n",
    "\n",
    "We now use the **Multiple word expression** tokenizer as we have to inlcude the Bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwetokenizer = MWETokenizer(top_200_bigrams)\n",
    "\n",
    "dict_tokens =  dict((date, mwetokenizer.tokenize(unigrams_list)) for date,unigrams_list in dict_stemmed_unigram_tokens.items())\n",
    "\n",
    "#print(dict_tokens)\n",
    "\n",
    "#print(dict_tokens.values())\n",
    "\n",
    "all_tokens_colloc = list(chain.from_iterable(dict_tokens.values()))\n",
    "\n",
    "list_colloc_voc = list(set(all_tokens_colloc))\n",
    "\n",
    "#print(list_colloc_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Generate Sparse representation using CountVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization we can understand that most of the tokens will not appear in all the files. So keeping a count of these tokens which have a count of 0 will add to the memory and processing overhead. So the matrix of document v/s count will be sparse as for most of the document the value will be zero.\n",
    "\n",
    "To solve this probem, we maintain a feature vector. Applying `fit_transform` to the new vocab. \n",
    "\n",
    "`fit_transform` does two things: First, it fits the model and learns the vocabulary; second it transforms the text data into feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT VECTOR\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "data_features = vectorizer.fit_transform(list([' '.join(value) for value in dict_tokens.values()]))\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Generate Vocabulary file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maintain an dictionary mapping of each Token and assign an index to it. This index and token is sored in the vocab file.\n",
    "\n",
    "The format of vocab file is : `token:index`\n",
    "\n",
    "Finally writing the tokens to a vocab file `31187366_vocab.txt` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the VOCAB file\n",
    "out_file = open(\"./final_op/31187366_vocab.txt\", 'w')\n",
    "\n",
    "i = 0\n",
    "\n",
    "tokens_list = list(sorted(list(set(vocab))))\n",
    "\n",
    "# get the single token\n",
    "for token in tokens_list:\n",
    "\n",
    "    # give each token an index\n",
    "    vocab_dict[token] = i\n",
    "    \n",
    "    out_file.write('{}:{}'.format(token,i) + '\\n')\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Generate Doc-Term Matrix file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the vocabulary, each document can be represented as a sequence of integers that correspond to the tokens,\n",
    "or in the following sparse form:\n",
    "```\n",
    "    word_index:word count\n",
    "```\n",
    "\n",
    "We keep count of only those tokens that have `count > 0` to solve the problem of sparse matrix.\n",
    "\n",
    "The `data_features` returns a matrix of count of Documents v/s Token. Each row is count of the token for a single token.\n",
    "\n",
    "So we can bind the together for each doument using the `zip` function.\n",
    "\n",
    "Finlly the result is written to `31187366_countVec.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to countVec file\n",
    "# open a file to write\n",
    "out_file = open(\"./final_op/31187366_countVec.txt\", 'w')\n",
    "\n",
    "i=0\n",
    "dict_count_token_each_day = {}\n",
    "\n",
    "while i < len(sheets): \n",
    "    count_token_each_doc = data_features.toarray()[i]\n",
    "\n",
    "    out_file.write(sheets[i] + \",\")    \n",
    "    for word, count in zip(vocab, count_token_each_doc):\n",
    "        \n",
    "        if count > 0:\n",
    "            #print (vocab_dict[word], \":\", count)\n",
    "            out_file.write(\"{}:{},\".format(vocab_dict[word],count))\n",
    "    out_file.write('\\n')\n",
    "    i = i+1\n",
    "    \n",
    "# close the file\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task help build up the knowledge of processing semi structured text, and converting the text to suitable format useful for downstream text analysis algorithms. The main outcomes achieved are as follows:\n",
    "\n",
    "- **Reading Excel file and parsing various sheets** : Using the pandas ReadExcel function we could read the given excel file and work on different sheets using the parse() function.\n",
    "\n",
    "- **DataFrame manipulation** : Storing the data in Pandas dataframe and manipulating it to get the text was important to get started with the task.\n",
    "\n",
    "- **Tokenization** - Converting each file text to String and tokenizing the words was important to get the Unigrams, Bigrams and Collocations from the data.\n",
    "\n",
    "- **Stemming** - Using the PorterStemmer we could see the effect of Stemming on the tokens. The conversion of words to their root form helped shorten the vocab.\n",
    "\n",
    "- **Generating Count Vector** - It is imortant to store the count of each token for analysis. As storing the count of each token fpr each file creates a Sparse Marix, we use count vetor form to save the count.\n",
    "\n",
    "- **Storing Data to specific format** - The process of storing the unigram and bigrams to external file helped to learn the storage of data to different format.\n",
    "\n",
    "- **Manipulating Python Data structures**: For successful completion of this task knowledge of manipulating basic Data Structures like `list`, `tuple` and `dictionary` was important. Using dictionary functions like `items` and `keys` was helpfult for iteration. Also the `in` operator was needed for various condition check.\n",
    "\n",
    "- **Remove Non-English Tweets** : Using `langid` package it was possible to check if the text of Tweet was in English or not. This was achieved by using the `classify` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk.collocations — NLTK 3.5 documentation. (2020). Retrieved 16 September 2020, from https://www.nltk.org/_modules/nltk/collocations.html\n",
    "- langid. (2020). Retrieved 15 September 2020, from https://pypi.org/project/langid/1.1dev/\n",
    "- 10 Minutes to pandas — pandas 0.22.0 documentation. (2020). Retrieved 16 September 2020, from https://pandas.pydata.org/pandas-docs/version/0.22.0/10min.html\n",
    "- Porter Stemming Algorithm. (2020). Retrieved 16 September 2020, from https://tartarus.org/martin/PorterStemmer/\n",
    "- Stemming. (2020). Retrieved 16 September 2020, from https://en.wikipedia.org/wiki/Stemming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
